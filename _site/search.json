[
  {
    "objectID": "1_EvaluationMetrics.html",
    "href": "1_EvaluationMetrics.html",
    "title": "",
    "section": "",
    "text": "CodeEvaluate performance in regression\n\n# Load required libraries\nlibrary(mlbench)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\n# Load the dataset and view\ndata(\"PimaIndiansDiabetes\")\nhead(PimaIndiansDiabetes)\n\n  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1        6     148       72      35       0 33.6    0.627  50      pos\n2        1      85       66      29       0 26.6    0.351  31      neg\n3        8     183       64       0       0 23.3    0.672  32      pos\n4        1      89       66      23      94 28.1    0.167  21      neg\n5        0     137       40      35     168 43.1    2.288  33      pos\n6        5     116       74       0       0 25.6    0.201  30      neg\n\n#Lets keep only the variables we are interested in \n\nPimaIndiansDiabetes_Small &lt;- PimaIndiansDiabetes %&gt;%\n  select(age, glucose, diabetes, mass)\n\n\n# Split into training and testing datasets\nset.seed(123)\n\ndata_split &lt;- initial_split(PimaIndiansDiabetes_Small, prop = 0.7)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\nCheck dimensions of both! Want to split original dataset in a 70%, 30%\n\ndim(PimaIndiansDiabetes_Small)\n\n[1] 768   4\n\ndim(train_data)\n\n[1] 537   4\n\ndim(test_data)\n\n[1] 231   4\n\n\nNow imagine we believe that the relationship between glucose and age is cubic. We can fit a polynomial like in class:\n\nmodel_cubic &lt;- lm(glucose ~ poly(age, 3), data = train_data)\nsummary(model_cubic)\n\n\nCall:\nlm(formula = glucose ~ poly(age, 3), data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-128.659  -20.660   -3.134   18.239   85.960 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    121.663      1.336  91.044  &lt; 2e-16 ***\npoly(age, 3)1  183.577     30.967   5.928 5.51e-09 ***\npoly(age, 3)2  -35.517     30.967  -1.147    0.252    \npoly(age, 3)3   -5.328     30.967  -0.172    0.863    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.97 on 533 degrees of freedom\nMultiple R-squared:  0.06407,   Adjusted R-squared:  0.05881 \nF-statistic: 12.16 on 3 and 533 DF,  p-value: 1.039e-07\n\n\nNow, we want to understand if the model will be able to generalize to unseen data. In other words, we want to understand if we are overfitting or underfitting our data. To study this, we are going to evaluate the performance of our model in both our training (where the model is fit, and best fit parameters determined) and our testing data (not part of the fitting data at all!)\nAs this is a regression model, we can calculate our well-known metrics RMSE, MSE, SSR .. that measure the error between predicted y (glucose) and actual glucose value. We can do this by calculating the metrics ourselves, or using the many already determined functions.\nBut first, lets make our predictions:\n\ntraining_glucose_prediction &lt;-  as.numeric(predict(model_cubic, train_data))\n\nhead(training_glucose_prediction) \n\n[1] 112.1167 127.2420 132.4825 112.1167 129.3420 115.7673\n\n\nTo make it easier for us, lets include this predicted glucose in our training data as an extra column\n\ntrain_data &lt;- cbind(train_data, pred_glucose =training_glucose_prediction )\n\nhead(train_data)\n\n    age glucose diabetes mass pred_glucose\n415  21     138      pos 34.6     112.1167\n463  39      74      neg 35.3     127.2420\n179  47     143      neg 45.0     132.4825\n526  21      87      neg 21.8     112.1167\n195  42      85      neg 24.4     129.3420\n118  25      78      neg 33.7     115.7673\n\n\nNow lets do the same for the testing data\n\ntesting_glucose_prediction &lt;-  as.numeric(predict(model_cubic, test_data))\n\ntest_data &lt;- cbind(test_data, pred_glucose = testing_glucose_prediction )\n\nhead(test_data)\n\n   age glucose diabetes mass pred_glucose\n1   50     148      pos 33.6     134.1276\n3   32     183      pos 23.3     121.8059\n4   21      89      neg 28.1     112.1167\n9   53     197      pos 30.5     135.5756\n15  51     166      pos 25.8     134.6329\n17  31     118      pos 45.8     120.9758\n\n\nOnto evaluating our performance:\n\n# Function to calculate RMSE\nrmse_mine &lt;- function(actual, predicted) {\n  sqrt(mean((actual - predicted)^2))\n}\n\nTrain:\n\nrmse_mine(train_data$glucose, train_data$pred_glucose )\n\n[1] 30.85099\n\n\nTest:\n\nrmse_mine(test_data$glucose, test_data$pred_glucose )\n\n[1] 30.60295\n\n\nCan use already predefined functions (e.g Metrics package https://cran.r-project.org/web/packages/Metrics/Metrics.pdf)\n\nlibrary(Metrics)\n\n\nAttaching package: 'Metrics'\n\n\nThe following objects are masked from 'package:yardstick':\n\n    accuracy, mae, mape, mase, precision, recall, rmse, smape\n\n\n\n# Calculate individual metrics: train\nMetrics::rmse(train_data$glucose, train_data$pred_glucose ) # the dots mean that I want this function rmse, to come from this package! In case other functions are named the same way\n\n[1] 30.85099\n\nMetrics::mae(train_data$glucose, train_data$pred_glucose )\n\n[1] 23.68902\n\n\n\n# Calculate individual metrics: test\nMetrics::rmse(test_data$glucose, test_data$pred_glucose)\n\n[1] 30.60295\n\nMetrics::mae(test_data$glucose, test_data$pred_glucose )\n\n[1] 24.59446\n\n\nDo not worry about trying to replicate the plots!\n\n# Visualise models\nggplot(train_data, aes(x = age, y = glucose)) +\n  geom_point(color = \"red\", alpha = 0.6) +\n  geom_line(data = data.frame(\n    age = seq(min(PimaIndiansDiabetes_Small$age), max(PimaIndiansDiabetes_Small$age), length.out = 100),\n    glucose = predict(model_cubic, newdata = data.frame(age = seq(min(PimaIndiansDiabetes_Small$age), max(PimaIndiansDiabetes_Small$age), length.out = 100)))\n  ), aes(x = age, y = glucose), color = \"blue\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Degree 3 Fit\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n# Visualise models\nggplot(test_data, aes(x = age, y = glucose)) +\n  geom_point(color = \"grey\", alpha = 0.6) +\n  geom_line(data = data.frame(\n    age = seq(min(PimaIndiansDiabetes_Small$age), max(PimaIndiansDiabetes_Small$age), length.out = 100),\n    glucose = predict(model_cubic, newdata = data.frame(age = seq(min(PimaIndiansDiabetes_Small$age), max(PimaIndiansDiabetes_Small$age), length.out = 100)))\n  ), aes(x = age, y = glucose), color = \"blue\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Degree 3 Fit\") +\n  theme_minimal()\n\n\n\n\n\n\n\nThey both fit the data in quite a similar way ! Overfitting, not happening as similar performance in both train and test, but probably this is not the best model, underfitting as missing lots of insight and error is high. What can be the reason behind this? My go to, is that we should increase number of features, as trying to predict glucose with age is too simple and cannot capture the complexity of glucose behaviour.\n\nKey things to understand: - Dataset splitting - Fitting model in training - Evaluating performance of fitted model in both train and test to understand possible overfitting, underfitting issues - Extract insight from modeling and decide next steps\n\n\n\n\n Back to top",
    "crumbs": [
      "Overfitting, DT and RF",
      "Evaluate performance in regression"
    ]
  },
  {
    "objectID": "2_EvaluationMetrics.html",
    "href": "2_EvaluationMetrics.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2_EvaluationMetrics.html#include-more-features",
    "href": "2_EvaluationMetrics.html#include-more-features",
    "title": "",
    "section": "Include more features",
    "text": "Include more features\n\nhead(PimaIndiansDiabetes)\n\n  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1        6     148       72      35       0 33.6    0.627  50      pos\n2        1      85       66      29       0 26.6    0.351  31      neg\n3        8     183       64       0       0 23.3    0.672  32      pos\n4        1      89       66      23      94 28.1    0.167  21      neg\n5        0     137       40      35     168 43.1    2.288  33      pos\n6        5     116       74       0       0 25.6    0.201  30      neg\n\n\n\nset.seed(123)\n\ndata_split &lt;- initial_split(PimaIndiansDiabetes, prop = 0.7)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n\nmodel_class_more &lt;- glm(diabetes ~ age + pressure + triceps + mass + insulin, family = \"binomial\", data = PimaIndiansDiabetes)\nsummary(model_class_more)\n\n\nCall:\nglm(formula = diabetes ~ age + pressure + triceps + mass + insulin, \n    family = \"binomial\", data = PimaIndiansDiabetes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1545  -0.8808  -0.5765   1.0737   2.7118  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.128249   0.543035  -9.444  &lt; 2e-16 ***\nage          0.049228   0.007369   6.681 2.38e-11 ***\npressure    -0.008907   0.004768  -1.868  0.06176 .  \ntriceps     -0.007765   0.006060  -1.281  0.20006    \nmass         0.104439   0.013701   7.623 2.48e-14 ***\ninsulin      0.002179   0.000784   2.779  0.00545 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 863.81  on 762  degrees of freedom\nAIC: 875.81\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe have our fitted coefficients! What do they mean? We can have a guide of the association between outcome label and predictors\n\ntest_data &lt;-  test_data %&gt;% \n  mutate(pred_diab = as.numeric(predict(model_class_more, test_data, type = \"response\")), \n         pred_class =  as.factor(ifelse(pred_diab &gt; 0.5, \"pos\", \"neg\")))\n  \nhead(test_data)\n\n   pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1         6     148       72      35       0 33.6    0.627  50      pos\n3         8     183       64       0       0 23.3    0.672  32      pos\n4         1      89       66      23      94 28.1    0.167  21      neg\n9         2     197       70      45     543 30.5    0.158  53      pos\n15        5     166       72      19     175 25.8    0.587  51      pos\n17        0     118       84      47     230 45.8    0.551  31      pos\n   pred_diab pred_class\n1  0.4823141        neg\n3  0.1558332        neg\n4  0.1516937        neg\n9  0.7060717        pos\n15 0.4180845        neg\n17 0.6385543        pos\n\n\n\ntrain_data &lt;-  train_data %&gt;% \n  mutate(pred_diab = as.numeric(predict(model_class_more, train_data, type = \"response\")), \n         pred_class =  as.factor(ifelse(pred_diab &gt; 0.5, \"pos\", \"neg\")))\n  \nhead(train_data)\n\n    pregnant glucose pressure triceps insulin mass pedigree age diabetes\n415        0     138       60      35     167 34.6    0.534  21      pos\n463        8      74       70      40      49 35.3    0.705  39      neg\n179        5     143       78       0       0 45.0    0.190  47      neg\n526        3      87       60      18       0 21.8    0.444  21      neg\n195        8      85       55      20       0 24.4    0.136  42      neg\n118        5      78       48       0       0 33.7    0.654  25      neg\n     pred_diab pred_class\n415 0.28430312        neg\n463 0.41362731        neg\n179 0.76682847        pos\n526 0.07642942        neg\n195 0.23911451        neg\n118 0.30885372        neg\n\n\n\nclass_metrics &lt;- metric_set(accuracy, precision, recall, sens, specificity)\n\n\ntest_data %&gt;%\n  class_metrics(truth = diabetes, estimate = pred_class )\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.680\n2 precision   binary         0.713\n3 recall      binary         0.847\n4 sens        binary         0.847\n5 specificity binary         0.370\n\n\n\ntrain_data %&gt;%\n  class_metrics(truth = diabetes, estimate = pred_class )\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.698\n2 precision   binary         0.721\n3 recall      binary         0.877\n4 sens        binary         0.877\n5 specificity binary         0.364\n\n\nSlight increase in performance."
  },
  {
    "objectID": "3_Confusion.html",
    "href": "3_Confusion.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "3_Confusion.html#confusion-matrices",
    "href": "3_Confusion.html#confusion-matrices",
    "title": "",
    "section": "Confusion Matrices",
    "text": "Confusion Matrices\nOn Friday you saw a very rudimentary confusion matrix done with the function table() as we learnt with correlation matrices last Tuesday, we can integrate already developed functions and packages to enhance our analysis\nHere is a set of different confusion matrices and an exercise at the end to experiment with what happens when thresholds change.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(tidymodels)\nlibrary(ConfusionTableR) #probably will need to install it\n\nLets create a sample dataset - similar to the diabetes dataset with made up probabilities:\n\n# Example data with predicted probabilities\nset.seed(123)\nactual &lt;- factor(sample(c(\"pos\", \"neg\"), 20, replace = TRUE, prob = c(0.5, 0.5)))\npredicted_prob &lt;- runif(20) # Simulated predicted probabilities\n\nChoose your threshold (e.g 0.5 to start)\n\n# Adjust threshold and classify\nthreshold &lt;- 0.5 \npredicted &lt;- factor(ifelse(predicted_prob &gt;= threshold, \"pos\", \"neg\"))\n\nConfusion matrix using tidymodels\n\ndata &lt;- data.frame(actual = actual, predicted = predicted)\n\ncm &lt;- data %&gt;%\n  conf_mat(truth = actual, estimate = predicted)\n\ncm\n\n          Truth\nPrediction neg pos\n       neg   4   3\n       pos   5   8\n\n\n\nautoplot(cm, type = \"heatmap\")\n\n\n\n\n\nConfusionTableR::binary_visualiseR(train_labels = predicted,\n                                   truth_labels= actual,\n                                   class_label1 = \"Negative\", \n                                   class_label2 = \"Positive\",\n                                   quadrant_col1 = \"#28ACB4\", \n                                   quadrant_col2 = \"#4397D2\", \n                                   custom_title = \"Diabetes Confusion Matrix\", \n                                   text_col= \"black\")\n\n\n\n\nChange your threshold to 0.3, what changes?\nNow same thing with 0.8"
  },
  {
    "objectID": "4_Decision_Trees.html",
    "href": "4_Decision_Trees.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "4_Decision_Trees.html#fitting-a-classification-decision-tree",
    "href": "4_Decision_Trees.html#fitting-a-classification-decision-tree",
    "title": "",
    "section": "Fitting a classification decision tree",
    "text": "Fitting a classification decision tree\n\n# Load libraries\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(tidymodels)\nlibrary(mlbench)\nlibrary(tidyverse)\nlibrary(caret) # for confusionMatrix function!\n\n\n# Load the dataset and view\ndata(\"PimaIndiansDiabetes\")\nhead(PimaIndiansDiabetes)\n\n  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1        6     148       72      35       0 33.6    0.627  50      pos\n2        1      85       66      29       0 26.6    0.351  31      neg\n3        8     183       64       0       0 23.3    0.672  32      pos\n4        1      89       66      23      94 28.1    0.167  21      neg\n5        0     137       40      35     168 43.1    2.288  33      pos\n6        5     116       74       0       0 25.6    0.201  30      neg\n\nPimaIndiansDiabetes$diabetes &lt;- factor(PimaIndiansDiabetes$diabetes, levels = c(\"pos\", \"neg\"))  # Convert Outcome to a factor\n\n\n# Split into training and testing datasets\nset.seed(123)\n\ndata_split &lt;- initial_split(PimaIndiansDiabetes, prop = 0.7, strata = diabetes)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\nNow, lets train a basic decision tree, remember, no issue with preprocessing, can just include all other variables! Recall what all of these varaiables mean: https://search.r-project.org/CRAN/refmans/mlbench/html/PimaIndiansDiabetes.html\n\nbasic_tree &lt;- rpart(diabetes ~ ., data = train_data, method = \"class\")\nbasic_tree\n\nn= 537 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 537 187 neg (0.34823091 0.65176909)  \n    2) glucose&gt;=127.5 189  71 pos (0.62433862 0.37566138)  \n      4) mass&gt;=29.95 142  39 pos (0.72535211 0.27464789)  \n        8) glucose&gt;=157.5 62   6 pos (0.90322581 0.09677419) *\n        9) glucose&lt; 157.5 80  33 pos (0.58750000 0.41250000)  \n         18) pressure&lt; 61 12   0 pos (1.00000000 0.00000000) *\n         19) pressure&gt;=61 68  33 pos (0.51470588 0.48529412)  \n           38) age&gt;=30.5 42  14 pos (0.66666667 0.33333333)  \n             76) pedigree&gt;=0.415 25   4 pos (0.84000000 0.16000000) *\n             77) pedigree&lt; 0.415 17   7 neg (0.41176471 0.58823529) *\n           39) age&lt; 30.5 26   7 neg (0.26923077 0.73076923) *\n      5) mass&lt; 29.95 47  15 neg (0.31914894 0.68085106)  \n       10) age&gt;=27.5 31  14 neg (0.45161290 0.54838710)  \n         20) insulin&gt;=69.5 13   4 pos (0.69230769 0.30769231) *\n         21) insulin&lt; 69.5 18   5 neg (0.27777778 0.72222222) *\n       11) age&lt; 27.5 16   1 neg (0.06250000 0.93750000) *\n    3) glucose&lt; 127.5 348  69 neg (0.19827586 0.80172414)  \n      6) age&gt;=28.5 154  53 neg (0.34415584 0.65584416)  \n       12) mass&gt;=26.35 128  52 neg (0.40625000 0.59375000)  \n         24) pedigree&gt;=0.561 35  12 pos (0.65714286 0.34285714)  \n           48) glucose&gt;=96.5 26   6 pos (0.76923077 0.23076923) *\n           49) glucose&lt; 96.5 9   3 neg (0.33333333 0.66666667) *\n         25) pedigree&lt; 0.561 93  29 neg (0.31182796 0.68817204)  \n           50) triceps&lt; 27.5 54  22 neg (0.40740741 0.59259259)  \n            100) glucose&gt;=93.5 44  22 pos (0.50000000 0.50000000)  \n              200) pressure&lt; 84 36  15 pos (0.58333333 0.41666667)  \n                400) pregnant&lt; 7.5 25   7 pos (0.72000000 0.28000000)  \n                  800) pedigree&lt; 0.379 18   2 pos (0.88888889 0.11111111) *\n                  801) pedigree&gt;=0.379 7   2 neg (0.28571429 0.71428571) *\n                401) pregnant&gt;=7.5 11   3 neg (0.27272727 0.72727273) *\n              201) pressure&gt;=84 8   1 neg (0.12500000 0.87500000) *\n            101) glucose&lt; 93.5 10   0 neg (0.00000000 1.00000000) *\n           51) triceps&gt;=27.5 39   7 neg (0.17948718 0.82051282) *\n       13) mass&lt; 26.35 26   1 neg (0.03846154 0.96153846) *\n      7) age&lt; 28.5 194  16 neg (0.08247423 0.91752577) *\n\n\n\nrpart.plot(basic_tree, extra = 101)  # Plot the tree\n\n\n\n\nWhen predicting, what will we be obtaining? Predictions? Classes? As with the previous we can specify this:\n\n# Evaluate the basic model\npred &lt;- predict(basic_tree, train_data, type = \"prob\")\nhead(pred)\n\n          pos       neg\n4  0.08247423 0.9175258\n8  0.27272727 0.7272727\n11 0.12500000 0.8750000\n19 0.17948718 0.8205128\n28 0.08247423 0.9175258\n29 0.69230769 0.3076923\n\n\nBut lets stick with class (default threshold 0.5)\n\n# Evaluate the basic model\npred &lt;- predict(basic_tree, train_data, type = \"class\")\nhead(pred)\n\n  4   8  11  19  28  29 \nneg neg neg neg neg pos \nLevels: pos neg\n\n\n\nconf_matrix &lt;- confusionMatrix(pred, train_data$diabetes)\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction pos neg\n       pos 134  22\n       neg  53 328\n                                          \n               Accuracy : 0.8603          \n                 95% CI : (0.8281, 0.8885)\n    No Information Rate : 0.6518          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.68            \n                                          \n Mcnemar's Test P-Value : 0.000532        \n                                          \n            Sensitivity : 0.7166          \n            Specificity : 0.9371          \n         Pos Pred Value : 0.8590          \n         Neg Pred Value : 0.8609          \n             Prevalence : 0.3482          \n         Detection Rate : 0.2495          \n   Detection Prevalence : 0.2905          \n      Balanced Accuracy : 0.8269          \n                                          \n       'Positive' Class : pos             \n                                          \n\n\nNow same thing with test\n\n# Evaluate the basic model\npred &lt;- predict(basic_tree, test_data, type = \"class\")\n\nconf_matrix &lt;- confusionMatrix(pred, test_data$diabetes)\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction pos neg\n       pos  44  18\n       neg  37 132\n                                          \n               Accuracy : 0.7619          \n                 95% CI : (0.7016, 0.8153)\n    No Information Rate : 0.6494          \n    P-Value [Acc &gt; NIR] : 0.0001491       \n                                          \n                  Kappa : 0.4473          \n                                          \n Mcnemar's Test P-Value : 0.0152192       \n                                          \n            Sensitivity : 0.5432          \n            Specificity : 0.8800          \n         Pos Pred Value : 0.7097          \n         Neg Pred Value : 0.7811          \n             Prevalence : 0.3506          \n         Detection Rate : 0.1905          \n   Detection Prevalence : 0.2684          \n      Balanced Accuracy : 0.7116          \n                                          \n       'Positive' Class : pos             \n                                          \n\n\nAs expected, training is much higher than test! Overfitting happening. Can we do something about it by tuning hyperparameters?"
  },
  {
    "objectID": "4_Decision_Trees.html#hyperparameters",
    "href": "4_Decision_Trees.html#hyperparameters",
    "title": "",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nWe explained that in order to prevent overfitting, decision trees have stopping criteria in the form of hyperparameters. Explore different hyperparamter combinations and see the impact it has on the predictive model created.\nFirst chek out the hyperparameters available. All information is available from the function help https://cran.r-project.org/web/packages/rpart/rpart.pdf and go to rpart and rpart.control\n\n# Manually tune hyperparameters (example with maxdepth)\ntuned_tree_depth &lt;- rpart(diabetes ~ ., data = train_data, method = \"class\",\n                          control = rpart.control(maxdepth = 3))\n\nrpart.plot(tuned_tree_depth)  # Plot the tuned tree\n\n\n\n\nVery different!! What has happened? Lets evaluate the manually tuned model\n\npred_depth_train &lt;- predict(tuned_tree_depth, train_data, type = \"class\")\nconf_matrix_depth &lt;- confusionMatrix(pred_depth_train, train_data$diabetes)\nprint(conf_matrix_depth)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction pos neg\n       pos 103  39\n       neg  84 311\n                                         \n               Accuracy : 0.7709         \n                 95% CI : (0.733, 0.8058)\n    No Information Rate : 0.6518         \n    P-Value [Acc &gt; NIR] : 1.384e-09      \n                                         \n                  Kappa : 0.4655         \n                                         \n Mcnemar's Test P-Value : 7.268e-05      \n                                         \n            Sensitivity : 0.5508         \n            Specificity : 0.8886         \n         Pos Pred Value : 0.7254         \n         Neg Pred Value : 0.7873         \n             Prevalence : 0.3482         \n         Detection Rate : 0.1918         \n   Detection Prevalence : 0.2644         \n      Balanced Accuracy : 0.7197         \n                                         \n       'Positive' Class : pos            \n                                         \n\n\n\npred_depth_test &lt;- predict(tuned_tree_depth, test_data, type = \"class\")\nconf_matrix_depth &lt;- confusionMatrix(pred_depth_test, test_data$diabetes)\nprint(conf_matrix_depth)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction pos neg\n       pos  47  18\n       neg  34 132\n                                          \n               Accuracy : 0.7749          \n                 95% CI : (0.7155, 0.8271)\n    No Information Rate : 0.6494          \n    P-Value [Acc &gt; NIR] : 2.434e-05       \n                                          \n                  Kappa : 0.4822          \n                                          \n Mcnemar's Test P-Value : 0.03751         \n                                          \n            Sensitivity : 0.5802          \n            Specificity : 0.8800          \n         Pos Pred Value : 0.7231          \n         Neg Pred Value : 0.7952          \n             Prevalence : 0.3506          \n         Detection Rate : 0.2035          \n   Detection Prevalence : 0.2814          \n      Balanced Accuracy : 0.7301          \n                                          \n       'Positive' Class : pos             \n                                          \n\n\nAccuracy has reduced BUT now the model is generalizable, it does not capture noise anymore, but the inherent relationship in the data, that is present in the test set too.\nAttempt tuning another hyperparameter of your choice and see what happens. You can modify more than one at the same time!"
  },
  {
    "objectID": "4_Decision_Trees.html#hyperparameter-tuning",
    "href": "4_Decision_Trees.html#hyperparameter-tuning",
    "title": "",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\nHyperparameter tuning is part of our training process, as will determine the final predictive model. Going one by one to see which hyperparameter improves our training performance is tedious so, we can do a grid-search where we specify combinations of values we want our hyperparameters to explore:\n\n# Grid search for hyperparameter tuning \ngrid &lt;- expand.grid(maxdepth = 3:5,                   # Maximum depth\n                    minsplit = c(10, 20))          # Minimum splits\n\nView what this looks like!\n\nresults &lt;- data.frame()  # To store results\n\nfor (i in 1:nrow(grid)) {\n  \n  tree_model &lt;- rpart(diabetes ~ ., data = train_data, method = \"class\",\n                      control = rpart.control(maxdepth = grid$maxdepth[i],\n                                              minsplit = grid$minsplit[i]))\n  pred &lt;- predict(tree_model, train_data, type = \"class\")\n  accuracy &lt;- sum(pred == train_data$diabetes) / nrow(train_data)\n  results &lt;- rbind(results, cbind(grid[i, ], Accuracy = accuracy))\n}\n\n# Display sorted results\nresults &lt;- results[order(-results$Accuracy), ]\nprint(results)\n\n  maxdepth minsplit  Accuracy\n3        5       10 0.8342644\n6        5       20 0.8286778\n2        4       10 0.8007449\n5        4       20 0.8007449\n1        3       10 0.7709497\n4        3       20 0.7709497\n\n\nNow lets train the final model with the best hyperparameters\n\n# Train the final model with the best hyperparameters\nbest_params &lt;- results[1, ]\nfinal_tree &lt;- rpart(diabetes ~ ., data = train_data, method = \"class\",\n                    control = rpart.control(maxdepth = best_params$maxdepth,\n                                            minsplit = best_params$minsplit))\n\n\nrpart.plot(final_tree)\n\n\n\n\n\nfinal_pred &lt;- predict(final_tree, train_data, type = \"class\")\nfinal_conf_matrix &lt;- confusionMatrix(final_pred, train_data$diabetes)\nprint(final_conf_matrix)# Plot the final model\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction pos neg\n       pos 125  27\n       neg  62 323\n                                          \n               Accuracy : 0.8343          \n                 95% CI : (0.8001, 0.8647)\n    No Information Rate : 0.6518          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.6183          \n                                          \n Mcnemar's Test P-Value : 0.0003134       \n                                          \n            Sensitivity : 0.6684          \n            Specificity : 0.9229          \n         Pos Pred Value : 0.8224          \n         Neg Pred Value : 0.8390          \n             Prevalence : 0.3482          \n         Detection Rate : 0.2328          \n   Detection Prevalence : 0.2831          \n      Balanced Accuracy : 0.7957          \n                                          \n       'Positive' Class : pos             \n                                          \n\n\n\n# Evaluate the final model\nfinal_pred &lt;- predict(final_tree, test_data, type = \"class\")\nfinal_conf_matrix &lt;- confusionMatrix(final_pred, test_data$diabetes)\nprint(final_conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction pos neg\n       pos  48  14\n       neg  33 136\n                                          \n               Accuracy : 0.7965          \n                 95% CI : (0.7388, 0.8465)\n    No Information Rate : 0.6494          \n    P-Value [Acc &gt; NIR] : 7.342e-07       \n                                          \n                  Kappa : 0.5277          \n                                          \n Mcnemar's Test P-Value : 0.00865         \n                                          \n            Sensitivity : 0.5926          \n            Specificity : 0.9067          \n         Pos Pred Value : 0.7742          \n         Neg Pred Value : 0.8047          \n             Prevalence : 0.3506          \n         Detection Rate : 0.2078          \n   Detection Prevalence : 0.2684          \n      Balanced Accuracy : 0.7496          \n                                          \n       'Positive' Class : pos             \n                                          \n\n\nHyperparameter tuning is guided by performance metrics in the training set alone! Remember, the test set can never be part of the fitting process and hyperparameter tuning is.A different thing is that, once the predictive model has been defined, we then go and evaluate the performance in both datasets (with the test set results describing generalizability - what we are really after!)"
  },
  {
    "objectID": "5_RandomForest.html",
    "href": "5_RandomForest.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "5_RandomForest.html#fitting-a-classification-random-forest",
    "href": "5_RandomForest.html#fitting-a-classification-random-forest",
    "title": "",
    "section": "Fitting a classification random forest",
    "text": "Fitting a classification random forest\n\n# Load libraries\nlibrary(ranger) #random forest learning algorithm package! Although many others too\nlibrary(tidymodels)\nlibrary(mlbench)\nlibrary(tidyverse)\nlibrary(caret) #confusionMatrix function\n\n\n# Load the dataset and view\ndata(\"PimaIndiansDiabetes\")\nhead(PimaIndiansDiabetes)\n\n  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1        6     148       72      35       0 33.6    0.627  50      pos\n2        1      85       66      29       0 26.6    0.351  31      neg\n3        8     183       64       0       0 23.3    0.672  32      pos\n4        1      89       66      23      94 28.1    0.167  21      neg\n5        0     137       40      35     168 43.1    2.288  33      pos\n6        5     116       74       0       0 25.6    0.201  30      neg\n\nPimaIndiansDiabetes$diabetes &lt;- factor(PimaIndiansDiabetes$diabetes, levels = c(\"pos\", \"neg\"))  # Convert Outcome to a factor\n\n\n# Split into training and testing datasets\nset.seed(123)\n\ndata_split &lt;- initial_split(PimaIndiansDiabetes, prop = 0.7, strata = diabetes)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\nNow, lets train a basic random forest algorithm.\n\nbasic_tree &lt;- ranger(diabetes ~ ., data = train_data, seed = 123, mode = \"classification\") # Need this because random bootstrapping plus random feature splitting!\n\nWarning in ranger(diabetes ~ ., data = train_data, seed = 123, mode =\n\"classification\"): Unused arguments: mode\n\nbasic_tree\n\nRanger result\n\nCall:\n ranger(diabetes ~ ., data = train_data, seed = 123, mode = \"classification\") \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      537 \nNumber of independent variables:  8 \nMtry:                             2 \nTarget node size:                 1 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error:             23.09 % \n\n\nClass (default threshold 0.5)\n\n# Evaluate the basic model\npred &lt;- predict(basic_tree, train_data, type = \"response\")\nhead(pred)\n\n$predictions\n  [1] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n [19] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n [37] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n [55] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n [73] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n [91] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n[109] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n[127] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n[145] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n[163] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n[181] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n[199] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n[217] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n[235] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n[253] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n[271] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n[289] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n[307] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n[325] neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg neg\n[343] neg neg neg neg neg neg neg neg pos pos pos pos pos pos pos pos pos pos\n[361] pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos\n[379] pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos\n[397] pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos\n[415] pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos\n[433] pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos\n[451] pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos\n[469] pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos\n[487] pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos\n[505] pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos\n[523] pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos\nLevels: pos neg\n\n$num.trees\n[1] 500\n\n$num.independent.variables\n[1] 8\n\n$num.samples\n[1] 537\n\n$treetype\n[1] \"Classification\"\n\n\n\nconf_matrix &lt;- confusionMatrix(pred$predictions, train_data$diabetes)\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction pos neg\n       pos 187   0\n       neg   0 350\n                                     \n               Accuracy : 1          \n                 95% CI : (0.9932, 1)\n    No Information Rate : 0.6518     \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16  \n                                     \n                  Kappa : 1          \n                                     \n Mcnemar's Test P-Value : NA         \n                                     \n            Sensitivity : 1.0000     \n            Specificity : 1.0000     \n         Pos Pred Value : 1.0000     \n         Neg Pred Value : 1.0000     \n             Prevalence : 0.3482     \n         Detection Rate : 0.3482     \n   Detection Prevalence : 0.3482     \n      Balanced Accuracy : 1.0000     \n                                     \n       'Positive' Class : pos        \n                                     \n\n\nLook at that performance!!!\nNow same thing with test\n\n# Evaluate the basic model\npred &lt;- predict(basic_tree, test_data, type = \"response\")\n\nconf_matrix &lt;- confusionMatrix(pred$predictions, test_data$diabetes)\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction pos neg\n       pos  47  18\n       neg  34 132\n                                          \n               Accuracy : 0.7749          \n                 95% CI : (0.7155, 0.8271)\n    No Information Rate : 0.6494          \n    P-Value [Acc &gt; NIR] : 2.434e-05       \n                                          \n                  Kappa : 0.4822          \n                                          \n Mcnemar's Test P-Value : 0.03751         \n                                          \n            Sensitivity : 0.5802          \n            Specificity : 0.8800          \n         Pos Pred Value : 0.7231          \n         Neg Pred Value : 0.7952          \n             Prevalence : 0.3506          \n         Detection Rate : 0.2035          \n   Detection Prevalence : 0.2814          \n      Balanced Accuracy : 0.7301          \n                                          \n       'Positive' Class : pos             \n                                          \n\n\nThis is a perfect case of complete overfitting! Have to expore hyperparameter tuning!"
  },
  {
    "objectID": "5_RandomForest.html#hyperparameters",
    "href": "5_RandomForest.html#hyperparameters",
    "title": "",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nWe explained that in order to prevent overfitting, decision trees have stopping criteria in the form of hyperparameters. Explore different hyperparamter combinations and see the impact it has on the predictive model created.\nFirst chek out the hyperparameters available. All information is available from the function help https://cran.r-project.org/web/packages/ranger/ranger.pdf and go to ranger. We will focus on mtry and min.node.size\n\n# Manually tune hyperparameters (example with mtry)\ntuned_tree_depth &lt;- ranger(diabetes ~ ., data = train_data, \n                          mtry = 2,min.node.size = 50, mode = \"classification\" )\n\nWarning in ranger(diabetes ~ ., data = train_data, mtry = 2, min.node.size =\n50, : Unused arguments: mode\n\n\n\npred_depth_train &lt;- predict(tuned_tree_depth, train_data, type = \"response\")\nconf_matrix_depth &lt;- confusionMatrix(pred_depth_train$predictions, train_data$diabetes)\nprint(conf_matrix_depth)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction pos neg\n       pos 126  22\n       neg  61 328\n                                        \n               Accuracy : 0.8454        \n                 95% CI : (0.812, 0.875)\n    No Information Rate : 0.6518        \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16     \n                                        \n                  Kappa : 0.6421        \n                                        \n Mcnemar's Test P-Value : 3.032e-05     \n                                        \n            Sensitivity : 0.6738        \n            Specificity : 0.9371        \n         Pos Pred Value : 0.8514        \n         Neg Pred Value : 0.8432        \n             Prevalence : 0.3482        \n         Detection Rate : 0.2346        \n   Detection Prevalence : 0.2756        \n      Balanced Accuracy : 0.8055        \n                                        \n       'Positive' Class : pos           \n                                        \n\n\n\npred_depth_test &lt;- predict(tuned_tree_depth, test_data, type = \"response\")\nconf_matrix_depth &lt;- confusionMatrix(pred_depth_test$predictions, test_data$diabetes)\nprint(conf_matrix_depth)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction pos neg\n       pos  41  19\n       neg  40 131\n                                          \n               Accuracy : 0.7446          \n                 95% CI : (0.6833, 0.7995)\n    No Information Rate : 0.6494          \n    P-Value [Acc &gt; NIR] : 0.001219        \n                                          \n                  Kappa : 0.4036          \n                                          \n Mcnemar's Test P-Value : 0.009220        \n                                          \n            Sensitivity : 0.5062          \n            Specificity : 0.8733          \n         Pos Pred Value : 0.6833          \n         Neg Pred Value : 0.7661          \n             Prevalence : 0.3506          \n         Detection Rate : 0.1775          \n   Detection Prevalence : 0.2597          \n      Balanced Accuracy : 0.6898          \n                                          \n       'Positive' Class : pos             \n                                          \n\n\nReduced accuracy in training but didnt improve in testing. Continue trying with other combinations!"
  },
  {
    "objectID": "5_RandomForest.html#hyperparameter-tuning",
    "href": "5_RandomForest.html#hyperparameter-tuning",
    "title": "",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\nPipeline level\nWe are going to tune our parameters directly using the tidymodels framework. In scikit-learn you also have a Pipeline functionality. For more examples recommend https://juliasilge.com/blog/sf-trees-random-tuning/. This approach cab be followed with any other learning algorithm!\n\n# Define the Random Forest model with tunable parameters\n\nrf_model &lt;- rand_forest(\n  mtry = tune(),\n  trees = 500,\n  min_n = tune()\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\nTo check name of hyperparamters (remember model dependent!) can go here https://parsnip.tidymodels.org/reference/details_rand_forest_ranger.html\nDefine the formula in a pipeline fashion\n\n# Define the workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_model(rf_model) %&gt;%\n  add_formula(diabetes ~ .)\n\nSame as before\n\n# Create a grid of hyperparameters\nrf_grid &lt;- grid_regular(\n  mtry(range = c(2, 7)),\n  min_n(range = c(2, 10)),\n  levels = 5\n)\n\nhead(rf_grid)\n\n# A tibble: 6 × 2\n   mtry min_n\n  &lt;int&gt; &lt;int&gt;\n1     2     2\n2     3     2\n3     4     2\n4     5     2\n5     7     2\n6     2     4\n\n\n\ntuning_results &lt;- rf_workflow %&gt;%\n  tune_grid(\n    resamples = bootstraps(train_data, times = 1),  #all data - normally crossv alidation done here  - but will learn later about it!\n    grid = rf_grid,\n    metrics = metric_set(accuracy)\n  )\n\n# Extract the best parameters based on accuracy\nbest_params &lt;- select_best(tuning_results, \"accuracy\")\n\n# Finalise the workflow with the best parameters\nfinal_rf_workflow &lt;- finalize_workflow(rf_workflow, best_params)\n\n# Fit the final model on the training data\nfinal_rf_model &lt;- final_rf_workflow %&gt;%\n  fit(data = train_data)\n\nEvaluate in test\n\n# Evaluate the final model on the test data\nfinal_predictions &lt;- predict(final_rf_model, test_data) %&gt;%\n  bind_cols(test_data)\n\nfinal_metrics &lt;- final_predictions %&gt;%\n  metrics(truth = diabetes, estimate = .pred_class)\n\n# Display metrics\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.784\n2 kap      binary         0.502\n\n\nEvaluate in train\n\n# Evaluate the final model on the test data\nfinal_predictions &lt;- predict(final_rf_model, train_data) %&gt;%\n  bind_cols(train_data)\n\nfinal_metrics &lt;- final_predictions %&gt;%\n  metrics(truth = diabetes, estimate = .pred_class)\n\n# Display metrics\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.976\n2 kap      binary         0.946\n\n\nWhat benefits do you think pipelines have?"
  }
]