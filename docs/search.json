[
  {
    "objectID": "random_forest_tidymodels_exercise.html",
    "href": "random_forest_tidymodels_exercise.html",
    "title": "Random Forest Classification (tidymodels)",
    "section": "",
    "text": "In this exercise you will:\n\nFit a random forest classifier to the Pima Indians Diabetes data.\nTune key hyperparameters using the tidymodels framework.\nCompare training vs test performance.\nReflect on the benefits of using pipelines/workflows.",
    "crumbs": [
      "Overfitting, DT and RF",
      "Random Forest Classification (tidymodels)"
    ]
  },
  {
    "objectID": "random_forest_tidymodels_exercise.html#goal",
    "href": "random_forest_tidymodels_exercise.html#goal",
    "title": "Random Forest Classification (tidymodels)",
    "section": "",
    "text": "In this exercise you will:\n\nFit a random forest classifier to the Pima Indians Diabetes data.\nTune key hyperparameters using the tidymodels framework.\nCompare training vs test performance.\nReflect on the benefits of using pipelines/workflows.",
    "crumbs": [
      "Overfitting, DT and RF",
      "Random Forest Classification (tidymodels)"
    ]
  },
  {
    "objectID": "random_forest_tidymodels_exercise.html#setup",
    "href": "random_forest_tidymodels_exercise.html#setup",
    "title": "Random Forest Classification (tidymodels)",
    "section": "1. Setup",
    "text": "1. Setup\n\nlibrary(tidymodels)\nlibrary(mlbench)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nset.seed(123)",
    "crumbs": [
      "Overfitting, DT and RF",
      "Random Forest Classification (tidymodels)"
    ]
  },
  {
    "objectID": "random_forest_tidymodels_exercise.html#data-preparation-and-split",
    "href": "random_forest_tidymodels_exercise.html#data-preparation-and-split",
    "title": "Random Forest Classification (tidymodels)",
    "section": "2. Data preparation and split",
    "text": "2. Data preparation and split\nWe will use the PimaIndiansDiabetes dataset, with outcome diabetes (pos = diabetes, neg = no diabetes).\n\ndata(\"PimaIndiansDiabetes\")\n\npima &lt;- PimaIndiansDiabetes %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    diabetes = factor(diabetes, levels = c(\"pos\", \"neg\"))\n  )\n\nlevels(pima$diabetes)\n\n[1] \"pos\" \"neg\"\n\nhead(pima)\n\n# A tibble: 6 × 9\n  pregnant glucose pressure triceps insulin  mass pedigree   age diabetes\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n1        6     148       72      35       0  33.6    0.627    50 pos     \n2        1      85       66      29       0  26.6    0.351    31 neg     \n3        8     183       64       0       0  23.3    0.672    32 pos     \n4        1      89       66      23      94  28.1    0.167    21 neg     \n5        0     137       40      35     168  43.1    2.29     33 pos     \n6        5     116       74       0       0  25.6    0.201    30 neg     \n\n\nSplit into training (70%) and test (30%) sets, stratifying by the outcome so that the class balance is similar in both sets:\n\ndata_split &lt;- initial_split(pima, prop = 0.7, strata = diabetes)\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\ndim(pima)\n\n[1] 768   9\n\ndim(train_data)\n\n[1] 537   9\n\ndim(test_data)\n\n[1] 231   9",
    "crumbs": [
      "Overfitting, DT and RF",
      "Random Forest Classification (tidymodels)"
    ]
  },
  {
    "objectID": "random_forest_tidymodels_exercise.html#random-forest-model-with-tunable-hyperparameters",
    "href": "random_forest_tidymodels_exercise.html#random-forest-model-with-tunable-hyperparameters",
    "title": "Random Forest Classification (tidymodels)",
    "section": "3. Random forest model with tunable hyperparameters",
    "text": "3. Random forest model with tunable hyperparameters\nWe set up a random forest model where some hyperparameters are marked with tune():\n\n\nmtry → number of predictors randomly sampled at each split\n\n\nmin_n → minimum number of observations in a terminal node\n\n\ntrees → number of trees in the forest (we keep this fixed at 500)\n\nTo check what they mean remember to go to help pages: ,To check name of hyperparamters (remember model dependent!) can go here https://parsnip.tidymodels.org/reference/details_rand_forest_ranger.html\n\nrf_spec &lt;- rand_forest(\n  mtry  = tune(),\n  min_n = tune(),\n  trees = 500\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\nrf_spec\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 500\n  min_n = tune()\n\nComputational engine: ranger \n\n\nWe also define a recipe that uses all predictors:\n\nrf_rec &lt;- recipe(diabetes ~ ., data = train_data)\n\nNow combine model + recipe into a workflow:\n\nrf_wf &lt;- workflow() %&gt;%\n  add_model(rf_spec) %&gt;%\n  add_recipe(rf_rec)\n\nrf_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 500\n  min_n = tune()\n\nComputational engine: ranger",
    "crumbs": [
      "Overfitting, DT and RF",
      "Random Forest Classification (tidymodels)"
    ]
  },
  {
    "objectID": "random_forest_tidymodels_exercise.html#hyperparameter-grid-and-resamples",
    "href": "random_forest_tidymodels_exercise.html#hyperparameter-grid-and-resamples",
    "title": "Random Forest Classification (tidymodels)",
    "section": "4. Hyperparameter grid and resamples",
    "text": "4. Hyperparameter grid and resamples\nWe want to search over a grid of values for mtry and min_n:\n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(2L, 7L)),\n  min_n(range = c(2L, 10L)),\n  levels = 5\n)\n\nrf_grid\n\n# A tibble: 25 × 2\n    mtry min_n\n   &lt;int&gt; &lt;int&gt;\n 1     2     2\n 2     3     2\n 3     4     2\n 4     5     2\n 5     7     2\n 6     2     4\n 7     3     4\n 8     4     4\n 9     5     4\n10     7     4\n# ℹ 15 more rows",
    "crumbs": [
      "Overfitting, DT and RF",
      "Random Forest Classification (tidymodels)"
    ]
  },
  {
    "objectID": "random_forest_tidymodels_exercise.html#tune-the-random-forest-with-tune_grid",
    "href": "random_forest_tidymodels_exercise.html#tune-the-random-forest-with-tune_grid",
    "title": "Random Forest Classification (tidymodels)",
    "section": "5. Tune the random forest with tune_grid()\n",
    "text": "5. Tune the random forest with tune_grid()\n\nNow we run the tuning. For each row in rf_grid, tidymodels:\n\nfits the model,\ncomputes performance metrics,\nstores the mean performance for that combination.\n\n\nset.seed(123)\n\nrf_res &lt;- tune_grid(\n  rf_wf,\n  resamples = bootstraps(train_data, times = 1), #normally cross-validation here - but will learn about this later on.\n  grid      = rf_grid,\n  metrics   = yardstick::metric_set(yardstick::accuracy, yardstick::roc_auc)\n)\n\nrf_res\n\n# Tuning results\n# Bootstrap sampling \n# A tibble: 1 × 4\n  splits            id         .metrics          .notes          \n  &lt;list&gt;            &lt;chr&gt;      &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [537/191]&gt; Bootstrap1 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 4]&gt;\n\n\n5.1 Inspect and visualise the tuning results\nCollect tuning results:\n\nrf_metrics &lt;- collect_metrics(rf_res)\nrf_metrics\n\n# A tibble: 50 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config         \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1     2     2 accuracy binary     0.764     1      NA pre0_mod01_post0\n 2     2     2 roc_auc  binary     0.852     1      NA pre0_mod01_post0\n 3     2     4 accuracy binary     0.749     1      NA pre0_mod02_post0\n 4     2     4 roc_auc  binary     0.849     1      NA pre0_mod02_post0\n 5     2     6 accuracy binary     0.764     1      NA pre0_mod03_post0\n 6     2     6 roc_auc  binary     0.851     1      NA pre0_mod03_post0\n 7     2     8 accuracy binary     0.764     1      NA pre0_mod04_post0\n 8     2     8 roc_auc  binary     0.854     1      NA pre0_mod04_post0\n 9     2    10 accuracy binary     0.764     1      NA pre0_mod05_post0\n10     2    10 roc_auc  binary     0.853     1      NA pre0_mod05_post0\n# ℹ 40 more rows\n\n\n\nautoplot(rf_res)\n\n\n\n\n\n\n\nFilter by ROC AUC:\n\nrf_metrics_auc &lt;- rf_metrics %&gt;%\n  filter(.metric == \"roc_auc\")\n\nrf_metrics_auc\n\n# A tibble: 25 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config         \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1     2     2 roc_auc binary     0.852     1      NA pre0_mod01_post0\n 2     2     4 roc_auc binary     0.849     1      NA pre0_mod02_post0\n 3     2     6 roc_auc binary     0.851     1      NA pre0_mod03_post0\n 4     2     8 roc_auc binary     0.854     1      NA pre0_mod04_post0\n 5     2    10 roc_auc binary     0.853     1      NA pre0_mod05_post0\n 6     3     2 roc_auc binary     0.846     1      NA pre0_mod06_post0\n 7     3     4 roc_auc binary     0.846     1      NA pre0_mod07_post0\n 8     3     6 roc_auc binary     0.850     1      NA pre0_mod08_post0\n 9     3     8 roc_auc binary     0.852     1      NA pre0_mod09_post0\n10     3    10 roc_auc binary     0.850     1      NA pre0_mod10_post0\n# ℹ 15 more rows\n\n\nPlot performance across hyperparameters:\n\nautoplot(rf_res, metric = \"roc_auc\")\n\n\n\n\n\n\n\n\nExercise:\n- Which combinations of mtry and min_n seem to work best?\n- Do deeper / more flexible forests always perform better?",
    "crumbs": [
      "Overfitting, DT and RF",
      "Random Forest Classification (tidymodels)"
    ]
  },
  {
    "objectID": "random_forest_tidymodels_exercise.html#final-model-with-the-best-hyperparameters",
    "href": "random_forest_tidymodels_exercise.html#final-model-with-the-best-hyperparameters",
    "title": "Random Forest Classification (tidymodels)",
    "section": "6. Final model with the best hyperparameters",
    "text": "6. Final model with the best hyperparameters\nSelect the best hyperparameter combination according to ROC AUC:\n\nbest_rf &lt;- select_best(rf_res, metric = \"roc_auc\")\nbest_rf\n\n# A tibble: 1 × 3\n   mtry min_n .config         \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;           \n1     2     8 pre0_mod04_post0\n\n\nFinalise the workflow and fit one final model on the full training data:\n\nfinal_rf_wf &lt;- finalize_workflow(rf_wf, best_rf)\n\nfinal_rf_fit &lt;- final_rf_wf %&gt;%\n  fit(data = train_data)\n\nfinal_rf_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~2L,      x), num.trees = ~500, min.node.size = min_rows(~8L, x), num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      537 \nNumber of independent variables:  8 \nMtry:                             2 \nTarget node size:                 8 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1592148",
    "crumbs": [
      "Overfitting, DT and RF",
      "Random Forest Classification (tidymodels)"
    ]
  },
  {
    "objectID": "random_forest_tidymodels_exercise.html#evaluate-the-final-random-forest",
    "href": "random_forest_tidymodels_exercise.html#evaluate-the-final-random-forest",
    "title": "Random Forest Classification (tidymodels)",
    "section": "7. Evaluate the final random forest",
    "text": "7. Evaluate the final random forest\nWe now compare training and test performance.\n\n# Training predictions\ntrain_preds &lt;- predict(final_rf_fit, train_data, type = \"prob\") %&gt;%\n  bind_cols(predict(final_rf_fit, train_data, type = \"class\")) %&gt;%\n  bind_cols(train_data)\n\n# Test predictions\ntest_preds &lt;- predict(final_rf_fit, test_data, type = \"prob\") %&gt;%\n  bind_cols(predict(final_rf_fit, test_data, type = \"class\")) %&gt;%\n  bind_cols(test_data)\n\nDefine a metric set:\n\nclass_metrics &lt;- yardstick::metric_set(\n  yardstick::accuracy,\n  yardstick::precision,\n  yardstick::recall,\n  yardstick::sens,\n  yardstick::specificity\n)\n\nCompute metrics:\n\ntrain_metrics &lt;- train_preds %&gt;%\n  class_metrics(truth = diabetes, estimate = .pred_class)\n\ntest_metrics &lt;- test_preds %&gt;%\n  class_metrics(truth = diabetes, estimate = .pred_class)\n\ntrain_metrics\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.978\n2 precision   binary         0.983\n3 recall      binary         0.952\n4 sens        binary         0.952\n5 specificity binary         0.991\n\ntest_metrics\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.749\n2 precision   binary         0.683\n3 recall      binary         0.531\n4 sens        binary         0.531\n5 specificity binary         0.867\n\n\nAlso compare ROC AUC:\n\nroc_auc(train_preds, truth = diabetes, .pred_pos)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.999\n\nroc_auc(test_preds,  truth = diabetes, .pred_pos)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.820\n\n\n\nExercise:\n- Is there still a big gap between training and test performance?\n- Compared to a basic (untuned) random forest, has the gap reduced?\n\nExtra:\nNow replicate this in Python, be sure to include these key functions:\n\nrf_basic = RandomForestClassifier( random_state=123 )\nrf_basic.fit(X_train, y_train)\naccuracy_score\nroc_auc_score\npipe = Pipeline( steps=[ (“rf”, RandomForestClassifier(random_state=123)) ] )\nparam_grid = { “rf__n_estimators”: [200], # keep fixed “rf__max_depth”: [None, 5]#, # shallow vs full depth “rf__min_samples_leaf”: [1, 5] # normal vs slightly regularised }\ngrid = GridSearchCV( estimator=pipe, param_grid=param_grid, cv=5,# before had just a single split of data here we are adding the 5 different splits! scoring=“roc_auc”, n_jobs=-1, refit=True, )\ngrid.fit(X_train, y_train)\nconfusion_matrix\nclassification_report",
    "crumbs": [
      "Overfitting, DT and RF",
      "Random Forest Classification (tidymodels)"
    ]
  },
  {
    "objectID": "random_forest_tidymodels_exercise.html#reflection-why-pipelines",
    "href": "random_forest_tidymodels_exercise.html#reflection-why-pipelines",
    "title": "Random Forest Classification (tidymodels)",
    "section": "8. Reflection – why pipelines?",
    "text": "8. Reflection – why pipelines?\n\nQuestions to think about:\n\nWhat benefits do you think workflows/pipelines have compared to writing everything by hand (e.g. separate model, formula, and prediction code)?\nHow does using a workflow help when:\n\nyou change the model (e.g. tree → random forest → logistic regression)?\n\nyou add preprocessing steps (e.g. scaling, imputation, dummy variables)?",
    "crumbs": [
      "Overfitting, DT and RF",
      "Random Forest Classification (tidymodels)"
    ]
  },
  {
    "objectID": "Classification_Performance_tidymodels_ROC.html",
    "href": "Classification_Performance_tidymodels_ROC.html",
    "title": "Evaluate Performance in Classification (tidymodels)",
    "section": "",
    "text": "# Load required libraries\nlibrary(mlbench)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(patchwork)\n\ntheme_set(theme_bw())\nset.seed(123)\nWe will work with the PimaIndiansDiabetes dataset and:\n# Load the dataset and keep only variables of interest\ndata(\"PimaIndiansDiabetes\")\n\npima_small &lt;- PimaIndiansDiabetes %&gt;%\n  as_tibble() %&gt;%\n  select(age, glucose, mass, diabetes) %&gt;%\n  mutate(\n    diabetes = factor(diabetes),          # ensure factor, not 0/1\n    diabetes = forcats::fct_relevel(diabetes, \"pos\", \"neg\")\n  )\n\nglimpse(pima_small)\n\nRows: 768\nColumns: 4\n$ age      &lt;dbl&gt; 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3…\n$ glucose  &lt;dbl&gt; 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,…\n$ mass     &lt;dbl&gt; 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, 0.0, 37…\n$ diabetes &lt;fct&gt; pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n…\nAlways key to see number of patients of each type\ntable(PimaIndiansDiabetes$diabetes)\n\n\nneg pos \n500 268",
    "crumbs": [
      "Overfitting, DT and RF",
      "Evaluate Performance in Classification (tidymodels)"
    ]
  },
  {
    "objectID": "Classification_Performance_tidymodels_ROC.html#include-more-features",
    "href": "Classification_Performance_tidymodels_ROC.html#include-more-features",
    "title": "Evaluate Performance in Classification (tidymodels)",
    "section": "Include more features",
    "text": "Include more features\nSo far we used only age. We now include additional predictors to see if performance improves.\n\npima_big &lt;- PimaIndiansDiabetes %&gt;%\n  mutate(\n    diabetes = factor(diabetes),          # ensure factor, not 0/1\n    diabetes = forcats::fct_relevel(diabetes, \"pos\", \"neg\")\n  )\n\nWe’ll now work with the full dataset and split again.\n\nset.seed(123)\n\ndata_split2 &lt;- initial_split(pima_big, prop = 0.7, strata = diabetes)\ntrain_data2 &lt;- training(data_split2)\ntest_data2  &lt;- testing(data_split2)\n\ndim(train_data2)\n\n[1] 537   9\n\ndim(test_data2)\n\n[1] 231   9\n\n\n1. New recipe with more features\nWe include multiple predictors in our recipe and also add simple preprocessing (e.g. median imputation and normalisation):\n\n# Model specification stays the same\nlog_spec2 &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\n# Recipe with more predictors\nlog_rec2 &lt;- recipe(diabetes ~ age + pressure + triceps + mass + insulin,\n                   data = train_data2) %&gt;%\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nlog_rec2\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5\n\n\n\n\n\n── Operations \n\n\n• Median imputation for: all_numeric_predictors()\n\n\n• Centering and scaling for: all_numeric_predictors()\n\n\nCreate the workflow and fit on training data:\n\nlog_wf2 &lt;- workflow() %&gt;%\n  add_model(log_spec2) %&gt;%\n  add_recipe(log_rec2)\n\nlog_fit2 &lt;- log_wf2 %&gt;%\n  fit(data = train_data2)\n\nlog_fit2\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_median()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)          age     pressure      triceps         mass      insulin  \n     0.7674      -0.5977       0.2135       0.2071      -0.8688      -0.3736  \n\nDegrees of Freedom: 536 Total (i.e. Null);  531 Residual\nNull Deviance:      694.2 \nResidual Deviance: 592.3    AIC: 604.3\n\n\nLook at the coefficients:\n\nlog_fit2 %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy()\n\n# A tibble: 6 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.767     0.104      7.39 1.45e-13\n2 age           -0.598     0.105     -5.70 1.19e- 8\n3 pressure       0.214     0.111      1.92 5.46e- 2\n4 triceps        0.207     0.120      1.73 8.38e- 2\n5 mass          -0.869     0.132     -6.60 4.17e-11\n6 insulin       -0.374     0.115     -3.24 1.20e- 3\n\n\n2. Predictions and performance (train & test)\n\ntrain_preds2 &lt;- predict(log_fit2, train_data2, type = \"prob\") %&gt;%\n  bind_cols(train_data2) %&gt;%\n  mutate(\n    pred_class = if_else(.pred_pos &gt; 0.5, \"pos\", \"neg\"),\n    pred_class = factor(pred_class, levels = levels(diabetes))\n  )\n\ntest_preds2 &lt;- predict(log_fit2, test_data2, type = \"prob\") %&gt;%\n  bind_cols(test_data2) %&gt;%\n  mutate(\n    pred_class = if_else(.pred_pos &gt; 0.5, \"pos\", \"neg\"),\n    pred_class = factor(pred_class, levels = levels(diabetes))\n  )\n\nTrain metrics\n\ntrain_preds2 %&gt;%\n  class_metrics(truth = diabetes, estimate = pred_class)\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.696\n2 precision   binary         0.602\n3 recall      binary         0.380\n4 sens        binary         0.380\n5 specificity binary         0.866\n\n\nTest metrics\n\ntest_preds2 %&gt;%\n  class_metrics(truth = diabetes, estimate = pred_class)\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.667\n2 precision   binary         0.548\n3 recall      binary         0.284\n4 sens        binary         0.284\n5 specificity binary         0.873\n\n\n3. ROC curve and AUC for the richer model (test set)\n\nroc_rich &lt;- roc_curve(test_preds2, truth = diabetes, .pred_pos)\nauc_rich &lt;- roc_auc(test_preds2, truth = diabetes, .pred_pos)\n\nauc_rich\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.715\n\n\n\nautoplot(roc_rich) +\n  ggtitle(\"ROC curve – multi-feature logistic model (test set)\")\n\n\n\n\n\n\n\nYou should observe:\n\nA higher AUC compared to the age-only model,\n\nA ROC curve that is further from the diagonal.\n\nYou will learn more about AUC and AUROC on Friday, but here is a very good visual of its meaning: https://mlu-explain.github.io/roc-auc/\nKey things to understand\n\n\nDataset splitting (train vs test) is crucial to assess generalisation.\n\nWe fit the model only on the training data.\n\nWe then evaluate performance on both train and test to detect:\n\noverfitting (train ≪ test error),\nunderfitting (both high and similar),\ngood generalisation (both low and similar).\n\n\n\nIn classification, we:\n\nwork with probabilities first,\nchoose a threshold to define predicted classes,\nuse metrics such as accuracy, precision, recall, sensitivity, specificity, etc.,\n\nand use ROC curves and AUC to summarise performance across all thresholds.\n\n\n\nAdding more relevant features can improve performance, but we still need to watch for overfitting in more flexible models.",
    "crumbs": [
      "Overfitting, DT and RF",
      "Evaluate Performance in Classification (tidymodels)"
    ]
  },
  {
    "objectID": "1_EvaluationMetrics_tidymodels.html",
    "href": "1_EvaluationMetrics_tidymodels.html",
    "title": "Evaluate Performance in Regression (tidymodels)",
    "section": "",
    "text": "# Load required libraries\nlibrary(mlbench)\nlibrary(tidymodels)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nset.seed(123)\n\nWe will work with the PimaIndiansDiabetes dataset and:\n\nsplit into training and testing sets,\nfit a simple logistic regression model,\nobtain predicted glucose,\nevaluate performance on train and test\n\n\n\n# Load the dataset and view\ndata(\"PimaIndiansDiabetes\")\nhead(PimaIndiansDiabetes)\n\n  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1        6     148       72      35       0 33.6    0.627  50      pos\n2        1      85       66      29       0 26.6    0.351  31      neg\n3        8     183       64       0       0 23.3    0.672  32      pos\n4        1      89       66      23      94 28.1    0.167  21      neg\n5        0     137       40      35     168 43.1    2.288  33      pos\n6        5     116       74       0       0 25.6    0.201  30      neg\n\n#Lets keep only the variables we are interested in \n\nPimaIndiansDiabetes_Small &lt;- PimaIndiansDiabetes %&gt;%\n  select(age, glucose, diabetes, mass)\n\n\n# Split into training and testing datasets\nset.seed(123)\n\ndata_split &lt;- initial_split(PimaIndiansDiabetes_Small, prop = 0.7)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\nCheck dimensions of both! Want to split original dataset in a 70%, 30%\n\ndim(PimaIndiansDiabetes_Small)\n\n[1] 768   4\n\ndim(train_data)\n\n[1] 537   4\n\ndim(test_data)\n\n[1] 231   4\n\n\nNow imagine we believe that the relationship between glucose and age is cubic. We can fit a polynomial like in class:\n\ncubic_spec &lt;- linear_reg() %&gt;%\nset_engine(\"lm\")\n\ncubic_rec &lt;- recipe(glucose ~ age, data = train_data) %&gt;%\nstep_poly(age, degree = 3)\n\nHave you seen? As this is feature transformation, we have included it as part of the recepies! As if it were part of the pre-processing. Check out how the data changes by viewing the pre-processing transformation\n\nbake(cubic_rec %&gt;% \n       prep(), new_data = NULL)\n\n# A tibble: 537 × 4\n   glucose age_poly_1 age_poly_2 age_poly_3\n     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1     138  -0.0443       0.0469   -0.0485 \n 2      74   0.0214      -0.0491    0.0166 \n 3     143   0.0506      -0.0377   -0.0373 \n 4      87  -0.0443       0.0469   -0.0485 \n 5      85   0.0323      -0.0487   -0.00292\n 6      78  -0.0297       0.0110    0.00882\n 7     100   0.0469      -0.0409   -0.0307 \n 8     197  -0.00783     -0.0273    0.0408 \n 9     119  -0.000530    -0.0358    0.0404 \n10     189   0.0944       0.0420   -0.0616 \n# ℹ 527 more rows\n\n\n\ncubic_wf &lt;- workflow() %&gt;%\nadd_model(cubic_spec) %&gt;%\nadd_recipe(cubic_rec)\n\n\nmodel_cubic &lt;- cubic_wf %&gt;%\nfit(data = train_data)\n\nmodel_cubic\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)   age_poly_1   age_poly_2   age_poly_3  \n    121.663      183.577      -35.517       -5.328  \n\n\nNow, we want to understand if the model will be able to generalize to unseen data. In other words, we want to understand if we are overfitting or underfitting our data. To study this, we are going to evaluate the performance of our model in both our training (where the model is fit, and best fit parameters determined) and our testing data (not part of the fitting data at all!)\nAs this is a regression model, we can calculate our well-known metrics RMSE, MSE, SSR .. that measure the error between predicted y (glucose) and actual glucose value. We can do this by calculating the metrics ourselves, or using the many already determined functions.\nBut first, lets make our predictions:\n\ntraining_glucose_prediction &lt;-  as.numeric(predict(model_cubic, train_data)$.pred)\n\nhead(training_glucose_prediction) \n\n[1] 112.1167 127.2420 132.4825 112.1167 129.3420 115.7673\n\n\nTo make it easier for us, lets include this predicted glucose in our training data as an extra column\n\ntrain_data &lt;- cbind(train_data, pred_glucose =training_glucose_prediction )\n\nhead(train_data)\n\n    age glucose diabetes mass pred_glucose\n415  21     138      pos 34.6     112.1167\n463  39      74      neg 35.3     127.2420\n179  47     143      neg 45.0     132.4825\n526  21      87      neg 21.8     112.1167\n195  42      85      neg 24.4     129.3420\n118  25      78      neg 33.7     115.7673\n\n\nTip! You can do this automatically using the function augment\n\ntrain_data &lt;-  augment(model_cubic, train_data)\n\nNow lets do the same for the testing data\n\ntest_data &lt;- augment(model_cubic, test_data)\n\nhead(test_data)\n\n# A tibble: 6 × 6\n  .pred .resid   age glucose diabetes  mass\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;\n1  134.  13.9     50     148 pos       33.6\n2  122.  61.2     32     183 pos       23.3\n3  112. -23.1     21      89 neg       28.1\n4  136.  61.4     53     197 pos       30.5\n5  135.  31.4     51     166 pos       25.8\n6  121.  -2.98    31     118 pos       45.8\n\n\nOnto evaluating our performance:\n\n# Function to calculate RMSE\nrmse_mine &lt;- function(actual, predicted) {\n  sqrt(mean((actual - predicted)^2))\n}\n\nTrain:\n\nrmse_mine(train_data$glucose, train_data$.pred )\n\n[1] 30.85099\n\n\nTest:\n\nrmse_mine(test_data$glucose, test_data$.pred )\n\n[1] 30.60295\n\n\nCan use already predefined functions (e.g Metrics package https://cran.r-project.org/web/packages/Metrics/Metrics.pdf)\n\nlibrary(Metrics)\n\n\nAttaching package: 'Metrics'\n\n\nThe following objects are masked from 'package:yardstick':\n\n    accuracy, mae, mape, mase, precision, recall, rmse, smape\n\n\n\n# Calculate individual metrics: train\nMetrics::rmse(train_data$glucose, train_data$.pred ) # the dots mean that I want this function rmse, to come from this package! In case other functions are named the same way\n\n[1] 30.85099\n\nMetrics::mae(train_data$glucose, train_data$.pred )\n\n[1] 23.68902\n\n\n\n# Calculate individual metrics: test\nMetrics::rmse(test_data$glucose, test_data$.pred)\n\n[1] 30.60295\n\nMetrics::mae(test_data$glucose, test_data$.pred )\n\n[1] 24.59446\n\n\nThey both fit the data in quite a similar way ! Overfitting, not happening as similar performance in both train and test, but probably this is not the best model, underfitting as missing lots of insight and error is high. What can be the reason behind this? My go to, is that we should increase number of features, as trying to predict glucose with age is too simple and cannot capture the complexity of glucose behaviour.\n\nKey things to understand: - Dataset splitting - Fitting model in training - Evaluating performance of fitted model in both train and test to understand possible overfitting, underfitting issues - Extract insight from modeling and decide next steps\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Overfitting, DT and RF",
      "Evaluate Performance in Regression (tidymodels)"
    ]
  },
  {
    "objectID": "DecisionTree_tidymodels_section.html",
    "href": "DecisionTree_tidymodels_section.html",
    "title": "Decision Trees in Classification (tidymodels)",
    "section": "",
    "text": "# Load libraries\nlibrary(tidymodels)\nlibrary(mlbench)\nlibrary(tidyverse)\nlibrary(rpart.plot)   # for plotting the fitted rpart tree\n\ntheme_set(theme_bw())\nset.seed(123)\n\n\n\n# Load the dataset and inspect\ndata(\"PimaIndiansDiabetes\")\n\npima &lt;- PimaIndiansDiabetes %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    diabetes = factor(diabetes),\n    diabetes = forcats::fct_relevel(diabetes, \"pos\", \"neg\") # make 'pos' the event\n  )\n\nlevels(pima$diabetes)\n\n[1] \"pos\" \"neg\"\n\nhead(pima)\n\n# A tibble: 6 × 9\n  pregnant glucose pressure triceps insulin  mass pedigree   age diabetes\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n1        6     148       72      35       0  33.6    0.627    50 pos     \n2        1      85       66      29       0  26.6    0.351    31 neg     \n3        8     183       64       0       0  23.3    0.672    32 pos     \n4        1      89       66      23      94  28.1    0.167    21 neg     \n5        0     137       40      35     168  43.1    2.29     33 pos     \n6        5     116       74       0       0  25.6    0.201    30 neg     \n\n\nWe split into training and testing datasets (70/30), stratifying by the outcome:\n\ndata_split &lt;- initial_split(pima, prop = 0.7, strata = diabetes)\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\ndim(pima)\n\n[1] 768   9\n\ndim(train_data)\n\n[1] 537   9\n\ndim(test_data)\n\n[1] 231   9\n\n\n\nWe start with a very simple tree model using all predictors.\n\n# Model specification: basic classification tree\ntree_spec_basic &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n# Recipe: use all predictors as-is\ntree_rec &lt;- recipe(diabetes ~ ., data = train_data)\n\n# Workflow: model + recipe\ntree_wf_basic &lt;- workflow() %&gt;%\n  add_model(tree_spec_basic) %&gt;%\n  add_recipe(tree_rec)\n\ntree_wf_basic\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nComputational engine: rpart \n\n\nFit the workflow on the training set:\n\ntree_fit_basic &lt;- tree_wf_basic %&gt;%\n  fit(data = train_data)\n\ntree_fit_basic\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 537 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 537 187 neg (0.34823091 0.65176909)  \n    2) glucose&gt;=127.5 189  71 pos (0.62433862 0.37566138)  \n      4) mass&gt;=29.95 142  39 pos (0.72535211 0.27464789)  \n        8) glucose&gt;=157.5 62   6 pos (0.90322581 0.09677419) *\n        9) glucose&lt; 157.5 80  33 pos (0.58750000 0.41250000)  \n         18) pressure&lt; 61 12   0 pos (1.00000000 0.00000000) *\n         19) pressure&gt;=61 68  33 pos (0.51470588 0.48529412)  \n           38) age&gt;=30.5 42  14 pos (0.66666667 0.33333333)  \n             76) pedigree&gt;=0.415 25   4 pos (0.84000000 0.16000000) *\n             77) pedigree&lt; 0.415 17   7 neg (0.41176471 0.58823529) *\n           39) age&lt; 30.5 26   7 neg (0.26923077 0.73076923) *\n      5) mass&lt; 29.95 47  15 neg (0.31914894 0.68085106)  \n       10) age&gt;=27.5 31  14 neg (0.45161290 0.54838710)  \n         20) insulin&gt;=69.5 13   4 pos (0.69230769 0.30769231) *\n         21) insulin&lt; 69.5 18   5 neg (0.27777778 0.72222222) *\n       11) age&lt; 27.5 16   1 neg (0.06250000 0.93750000) *\n    3) glucose&lt; 127.5 348  69 neg (0.19827586 0.80172414)  \n      6) age&gt;=28.5 154  53 neg (0.34415584 0.65584416)  \n       12) mass&gt;=26.35 128  52 neg (0.40625000 0.59375000)  \n         24) pedigree&gt;=0.561 35  12 pos (0.65714286 0.34285714)  \n           48) glucose&gt;=96.5 26   6 pos (0.76923077 0.23076923) *\n           49) glucose&lt; 96.5 9   3 neg (0.33333333 0.66666667) *\n         25) pedigree&lt; 0.561 93  29 neg (0.31182796 0.68817204)  \n           50) triceps&lt; 27.5 54  22 neg (0.40740741 0.59259259)  \n            100) glucose&gt;=93.5 44  22 pos (0.50000000 0.50000000)  \n              200) pressure&lt; 84 36  15 pos (0.58333333 0.41666667)  \n                400) pregnant&lt; 7.5 25   7 pos (0.72000000 0.28000000)  \n                  800) pedigree&lt; 0.379 18   2 pos (0.88888889 0.11111111) *\n                  801) pedigree&gt;=0.379 7   2 neg (0.28571429 0.71428571) *\n                401) pregnant&gt;=7.5 11   3 neg (0.27272727 0.72727273) *\n              201) pressure&gt;=84 8   1 neg (0.12500000 0.87500000) *\n            101) glucose&lt; 93.5 10   0 neg (0.00000000 1.00000000) *\n           51) triceps&gt;=27.5 39   7 neg (0.17948718 0.82051282) *\n       13) mass&lt; 26.35 26   1 neg (0.03846154 0.96153846) *\n      7) age&lt; 28.5 194  16 neg (0.08247423 0.91752577) *\n\n\n\nWe can extract the underlying rpart object and use rpart.plot():\n\ntree_fit_basic %&gt;%\n  extract_fit_parsnip() %&gt;%\n  pluck(\"fit\") %&gt;%\n  rpart.plot(extra = 101)\n\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\n\n\n\nWe now evaluate performance on train and test data.\n\n\ntrain_preds_basic &lt;- predict(tree_fit_basic, train_data, type = \"prob\") %&gt;%\n  bind_cols(predict(tree_fit_basic, train_data, type = \"class\")) %&gt;%\n  bind_cols(train_data)\n\nhead(train_preds_basic)\n\n# A tibble: 6 × 12\n  .pred_pos .pred_neg .pred_class pregnant glucose pressure triceps insulin\n      &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    0.0825     0.918 neg                1      89       66      23      94\n2    0.273      0.727 neg               10     115        0       0       0\n3    0.125      0.875 neg                4     110       92       0       0\n4    0.179      0.821 neg                1     103       30      38      83\n5    0.0825     0.918 neg                1      97       66      15     140\n6    0.692      0.308 pos               13     145       82      19     110\n# ℹ 4 more variables: mass &lt;dbl&gt;, pedigree &lt;dbl&gt;, age &lt;dbl&gt;, diabetes &lt;fct&gt;\n\n\nThe .pred_pos column contains the estimated probability of \"pos\" (diabetes present), and .pred_class is the predicted class using the default threshold 0.5.\n\n\ntest_preds_basic &lt;- predict(tree_fit_basic, test_data, type = \"prob\") %&gt;%\n  bind_cols(predict(tree_fit_basic, test_data, type = \"class\")) %&gt;%\n  bind_cols(test_data)\n\nhead(test_preds_basic)\n\n# A tibble: 6 × 12\n  .pred_pos .pred_neg .pred_class pregnant glucose pressure triceps insulin\n      &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    0.84       0.16  pos                6     148       72      35       0\n2    0.179      0.821 neg                1      85       66      29       0\n3    0.278      0.722 neg                8     183       64       0       0\n4    0.0385     0.962 neg                5     116       74       0       0\n5    0.278      0.722 neg               10     139       80       0       0\n6    0.692      0.308 pos                5     166       72      19     175\n# ℹ 4 more variables: mass &lt;dbl&gt;, pedigree &lt;dbl&gt;, age &lt;dbl&gt;, diabetes &lt;fct&gt;\n\n\n\nWe can use yardstick metrics (tidymodels) instead of caret::confusionMatrix.\n\nclass_metrics &lt;- yardstick::metric_set(\n  yardstick::accuracy,\n  yardstick::precision,\n  yardstick::recall,\n  yardstick::sens,\n  yardstick::specificity\n)\n\nTraining performance:\n\ntrain_conf_basic &lt;- conf_mat(train_preds_basic,\n                             truth = diabetes,\n                             estimate = .pred_class)\ntrain_conf_basic\n\n          Truth\nPrediction pos neg\n       pos 134  22\n       neg  53 328\n\n\n\ntrain_metrics_basic &lt;- train_preds_basic %&gt;%\n  class_metrics(truth = diabetes, estimate = .pred_class)\n\ntrain_metrics_basic\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.860\n2 precision   binary         0.859\n3 recall      binary         0.717\n4 sens        binary         0.717\n5 specificity binary         0.937\n\n\nTesting performance:\n\ntest_conf_basic &lt;- conf_mat(test_preds_basic,\n                            truth = diabetes,\n                            estimate = .pred_class)\ntest_conf_basic\n\n          Truth\nPrediction pos neg\n       pos  44  18\n       neg  37 132\n\n\n\ntest_metrics_basic &lt;- test_preds_basic %&gt;%\n  class_metrics(truth = diabetes, estimate = .pred_class)\n\ntest_metrics_basic\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.762\n2 precision   binary         0.710\n3 recall      binary         0.543\n4 sens        binary         0.543\n5 specificity binary         0.88 \n\n\nAs expected, training accuracy is higher than testing accuracy → the tree is starting to overfit the training data.",
    "crumbs": [
      "Overfitting, DT and RF",
      "Decision Trees in Classification (tidymodels)"
    ]
  },
  {
    "objectID": "DecisionTree_tidymodels_section.html#fitting-a-classification-decision-tree-tidymodels",
    "href": "DecisionTree_tidymodels_section.html#fitting-a-classification-decision-tree-tidymodels",
    "title": "Decision Trees in Classification (tidymodels)",
    "section": "",
    "text": "# Load libraries\nlibrary(tidymodels)\nlibrary(mlbench)\nlibrary(tidyverse)\nlibrary(rpart.plot)   # for plotting the fitted rpart tree\n\ntheme_set(theme_bw())\nset.seed(123)\n\n\n\n# Load the dataset and inspect\ndata(\"PimaIndiansDiabetes\")\n\npima &lt;- PimaIndiansDiabetes %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    diabetes = factor(diabetes),\n    diabetes = forcats::fct_relevel(diabetes, \"pos\", \"neg\") # make 'pos' the event\n  )\n\nlevels(pima$diabetes)\n\n[1] \"pos\" \"neg\"\n\nhead(pima)\n\n# A tibble: 6 × 9\n  pregnant glucose pressure triceps insulin  mass pedigree   age diabetes\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n1        6     148       72      35       0  33.6    0.627    50 pos     \n2        1      85       66      29       0  26.6    0.351    31 neg     \n3        8     183       64       0       0  23.3    0.672    32 pos     \n4        1      89       66      23      94  28.1    0.167    21 neg     \n5        0     137       40      35     168  43.1    2.29     33 pos     \n6        5     116       74       0       0  25.6    0.201    30 neg     \n\n\nWe split into training and testing datasets (70/30), stratifying by the outcome:\n\ndata_split &lt;- initial_split(pima, prop = 0.7, strata = diabetes)\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\ndim(pima)\n\n[1] 768   9\n\ndim(train_data)\n\n[1] 537   9\n\ndim(test_data)\n\n[1] 231   9\n\n\n\nWe start with a very simple tree model using all predictors.\n\n# Model specification: basic classification tree\ntree_spec_basic &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n# Recipe: use all predictors as-is\ntree_rec &lt;- recipe(diabetes ~ ., data = train_data)\n\n# Workflow: model + recipe\ntree_wf_basic &lt;- workflow() %&gt;%\n  add_model(tree_spec_basic) %&gt;%\n  add_recipe(tree_rec)\n\ntree_wf_basic\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nComputational engine: rpart \n\n\nFit the workflow on the training set:\n\ntree_fit_basic &lt;- tree_wf_basic %&gt;%\n  fit(data = train_data)\n\ntree_fit_basic\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 537 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 537 187 neg (0.34823091 0.65176909)  \n    2) glucose&gt;=127.5 189  71 pos (0.62433862 0.37566138)  \n      4) mass&gt;=29.95 142  39 pos (0.72535211 0.27464789)  \n        8) glucose&gt;=157.5 62   6 pos (0.90322581 0.09677419) *\n        9) glucose&lt; 157.5 80  33 pos (0.58750000 0.41250000)  \n         18) pressure&lt; 61 12   0 pos (1.00000000 0.00000000) *\n         19) pressure&gt;=61 68  33 pos (0.51470588 0.48529412)  \n           38) age&gt;=30.5 42  14 pos (0.66666667 0.33333333)  \n             76) pedigree&gt;=0.415 25   4 pos (0.84000000 0.16000000) *\n             77) pedigree&lt; 0.415 17   7 neg (0.41176471 0.58823529) *\n           39) age&lt; 30.5 26   7 neg (0.26923077 0.73076923) *\n      5) mass&lt; 29.95 47  15 neg (0.31914894 0.68085106)  \n       10) age&gt;=27.5 31  14 neg (0.45161290 0.54838710)  \n         20) insulin&gt;=69.5 13   4 pos (0.69230769 0.30769231) *\n         21) insulin&lt; 69.5 18   5 neg (0.27777778 0.72222222) *\n       11) age&lt; 27.5 16   1 neg (0.06250000 0.93750000) *\n    3) glucose&lt; 127.5 348  69 neg (0.19827586 0.80172414)  \n      6) age&gt;=28.5 154  53 neg (0.34415584 0.65584416)  \n       12) mass&gt;=26.35 128  52 neg (0.40625000 0.59375000)  \n         24) pedigree&gt;=0.561 35  12 pos (0.65714286 0.34285714)  \n           48) glucose&gt;=96.5 26   6 pos (0.76923077 0.23076923) *\n           49) glucose&lt; 96.5 9   3 neg (0.33333333 0.66666667) *\n         25) pedigree&lt; 0.561 93  29 neg (0.31182796 0.68817204)  \n           50) triceps&lt; 27.5 54  22 neg (0.40740741 0.59259259)  \n            100) glucose&gt;=93.5 44  22 pos (0.50000000 0.50000000)  \n              200) pressure&lt; 84 36  15 pos (0.58333333 0.41666667)  \n                400) pregnant&lt; 7.5 25   7 pos (0.72000000 0.28000000)  \n                  800) pedigree&lt; 0.379 18   2 pos (0.88888889 0.11111111) *\n                  801) pedigree&gt;=0.379 7   2 neg (0.28571429 0.71428571) *\n                401) pregnant&gt;=7.5 11   3 neg (0.27272727 0.72727273) *\n              201) pressure&gt;=84 8   1 neg (0.12500000 0.87500000) *\n            101) glucose&lt; 93.5 10   0 neg (0.00000000 1.00000000) *\n           51) triceps&gt;=27.5 39   7 neg (0.17948718 0.82051282) *\n       13) mass&lt; 26.35 26   1 neg (0.03846154 0.96153846) *\n      7) age&lt; 28.5 194  16 neg (0.08247423 0.91752577) *\n\n\n\nWe can extract the underlying rpart object and use rpart.plot():\n\ntree_fit_basic %&gt;%\n  extract_fit_parsnip() %&gt;%\n  pluck(\"fit\") %&gt;%\n  rpart.plot(extra = 101)\n\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\n\n\n\nWe now evaluate performance on train and test data.\n\n\ntrain_preds_basic &lt;- predict(tree_fit_basic, train_data, type = \"prob\") %&gt;%\n  bind_cols(predict(tree_fit_basic, train_data, type = \"class\")) %&gt;%\n  bind_cols(train_data)\n\nhead(train_preds_basic)\n\n# A tibble: 6 × 12\n  .pred_pos .pred_neg .pred_class pregnant glucose pressure triceps insulin\n      &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    0.0825     0.918 neg                1      89       66      23      94\n2    0.273      0.727 neg               10     115        0       0       0\n3    0.125      0.875 neg                4     110       92       0       0\n4    0.179      0.821 neg                1     103       30      38      83\n5    0.0825     0.918 neg                1      97       66      15     140\n6    0.692      0.308 pos               13     145       82      19     110\n# ℹ 4 more variables: mass &lt;dbl&gt;, pedigree &lt;dbl&gt;, age &lt;dbl&gt;, diabetes &lt;fct&gt;\n\n\nThe .pred_pos column contains the estimated probability of \"pos\" (diabetes present), and .pred_class is the predicted class using the default threshold 0.5.\n\n\ntest_preds_basic &lt;- predict(tree_fit_basic, test_data, type = \"prob\") %&gt;%\n  bind_cols(predict(tree_fit_basic, test_data, type = \"class\")) %&gt;%\n  bind_cols(test_data)\n\nhead(test_preds_basic)\n\n# A tibble: 6 × 12\n  .pred_pos .pred_neg .pred_class pregnant glucose pressure triceps insulin\n      &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    0.84       0.16  pos                6     148       72      35       0\n2    0.179      0.821 neg                1      85       66      29       0\n3    0.278      0.722 neg                8     183       64       0       0\n4    0.0385     0.962 neg                5     116       74       0       0\n5    0.278      0.722 neg               10     139       80       0       0\n6    0.692      0.308 pos                5     166       72      19     175\n# ℹ 4 more variables: mass &lt;dbl&gt;, pedigree &lt;dbl&gt;, age &lt;dbl&gt;, diabetes &lt;fct&gt;\n\n\n\nWe can use yardstick metrics (tidymodels) instead of caret::confusionMatrix.\n\nclass_metrics &lt;- yardstick::metric_set(\n  yardstick::accuracy,\n  yardstick::precision,\n  yardstick::recall,\n  yardstick::sens,\n  yardstick::specificity\n)\n\nTraining performance:\n\ntrain_conf_basic &lt;- conf_mat(train_preds_basic,\n                             truth = diabetes,\n                             estimate = .pred_class)\ntrain_conf_basic\n\n          Truth\nPrediction pos neg\n       pos 134  22\n       neg  53 328\n\n\n\ntrain_metrics_basic &lt;- train_preds_basic %&gt;%\n  class_metrics(truth = diabetes, estimate = .pred_class)\n\ntrain_metrics_basic\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.860\n2 precision   binary         0.859\n3 recall      binary         0.717\n4 sens        binary         0.717\n5 specificity binary         0.937\n\n\nTesting performance:\n\ntest_conf_basic &lt;- conf_mat(test_preds_basic,\n                            truth = diabetes,\n                            estimate = .pred_class)\ntest_conf_basic\n\n          Truth\nPrediction pos neg\n       pos  44  18\n       neg  37 132\n\n\n\ntest_metrics_basic &lt;- test_preds_basic %&gt;%\n  class_metrics(truth = diabetes, estimate = .pred_class)\n\ntest_metrics_basic\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.762\n2 precision   binary         0.710\n3 recall      binary         0.543\n4 sens        binary         0.543\n5 specificity binary         0.88 \n\n\nAs expected, training accuracy is higher than testing accuracy → the tree is starting to overfit the training data.",
    "crumbs": [
      "Overfitting, DT and RF",
      "Decision Trees in Classification (tidymodels)"
    ]
  },
  {
    "objectID": "DecisionTree_tidymodels_section.html#hyperparameters-manual-change",
    "href": "DecisionTree_tidymodels_section.html#hyperparameters-manual-change",
    "title": "Decision Trees in Classification (tidymodels)",
    "section": "Hyperparameters (manual change)",
    "text": "Hyperparameters (manual change)\nDecision trees have several stopping criteria (hyperparameters) that control complexity and help reduce overfitting. In tidymodels these are, for example:\n\n\ntree_depth – maximum depth of the tree\n\n\nmin_n – minimum number of samples in a terminal node\n\n\ncost_complexity – complexity parameter (cp) for pruning\n\nWe can manually set some of these and see the effect.\n\ntree_spec_shallow &lt;- decision_tree(\n  tree_depth = 3,\n  min_n = 20\n) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\ntree_wf_shallow &lt;- workflow() %&gt;%\n  add_model(tree_spec_shallow) %&gt;%\n  add_recipe(tree_rec)\n\ntree_fit_shallow &lt;- tree_wf_shallow %&gt;%\n  fit(data = train_data)\n\ntree_fit_shallow %&gt;%\n  extract_fit_parsnip() %&gt;%\n  pluck(\"fit\") %&gt;%\n  rpart.plot(extra = 101)\n\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\n\n\nEvaluate the shallow tree\n\ntrain_preds_shallow &lt;- predict(tree_fit_shallow, train_data, type = \"prob\") %&gt;%\n  bind_cols(predict(tree_fit_shallow, train_data, type = \"class\")) %&gt;%\n  bind_cols(train_data)\n\ntest_preds_shallow &lt;- predict(tree_fit_shallow, test_data, type = \"prob\") %&gt;%\n  bind_cols(predict(tree_fit_shallow, test_data, type = \"class\")) %&gt;%\n  bind_cols(test_data)\n\n\ntrain_metrics_shallow &lt;- train_preds_shallow %&gt;%\n  class_metrics(truth = diabetes, estimate = .pred_class)\n\ntest_metrics_shallow &lt;- test_preds_shallow %&gt;%\n  class_metrics(truth = diabetes, estimate = .pred_class)\n\ntrain_metrics_shallow\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.771\n2 precision   binary         0.725\n3 recall      binary         0.551\n4 sens        binary         0.551\n5 specificity binary         0.889\n\ntest_metrics_shallow\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.775\n2 precision   binary         0.723\n3 recall      binary         0.580\n4 sens        binary         0.580\n5 specificity binary         0.88 \n\n\nYou should see:\n\nTraining accuracy drops (the model is simpler),\n\nTesting accuracy is often closer to training, indicating better generalisation and less overfitting.\n\nYou can manually try other values of tree_depth and min_n and observe the effect.",
    "crumbs": [
      "Overfitting, DT and RF",
      "Decision Trees in Classification (tidymodels)"
    ]
  },
  {
    "objectID": "DecisionTree_tidymodels_section.html#hyperparameter-tuning-with-tidymodels",
    "href": "DecisionTree_tidymodels_section.html#hyperparameter-tuning-with-tidymodels",
    "title": "Decision Trees in Classification (tidymodels)",
    "section": "Hyperparameter tuning with tidymodels",
    "text": "Hyperparameter tuning with tidymodels\nInstead of trying values one-by-one, we can systematically search over a grid of hyperparameters using grid search. For more details: https://www.tmwr.org/grid-search\n1. Create a tunable tree specification\n\ntree_spec_tune &lt;- decision_tree(\n  tree_depth = tune(),\n  min_n      = tune()\n) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\ntree_spec_tune\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  tree_depth = tune()\n  min_n = tune()\n\nComputational engine: rpart \n\n\nWhich parameters to try? There are default parameters that you can check associated to the model libraries https://dials.tidymodels.org/articles/Basics.html For example we could create models with these range of values and see which ones works best:\n3. Define a tuning grid\n\ntree_grid &lt;- grid_regular(\n  tree_depth(range = c(1L, 10L)),\n  min_n(range = c(2L, 30L)),\n  levels = 5\n)\n\ntree_grid\n\n# A tibble: 25 × 2\n   tree_depth min_n\n        &lt;int&gt; &lt;int&gt;\n 1          1     2\n 2          3     2\n 3          5     2\n 4          7     2\n 5         10     2\n 6          1     9\n 7          3     9\n 8          5     9\n 9          7     9\n10         10     9\n# ℹ 15 more rows\n\n\n\ntree_wf_cv &lt;- workflow() %&gt;%\n  add_model(tree_spec_tune) %&gt;%\n  add_recipe(tree_rec)\n\n4. Run tune_grid()\n\nFor each combination in the grid, tidymodels fits a model and calculates accuracy and ROC AUC (metrics of choice for classification). We will see on Friday (and you saw yesterday) that the proper way of doing this is through cross-validation. Why is this needed? Because a single train/test split can give a lucky (or unlucky) result. Cross-validation gives a stable estimate of performance for each hyperparameter setting. But lets go one step at a time.\n\nset.seed(123)\n\n\ntree_res &lt;- tune_grid(\n  tree_wf_cv,\n  resamples = bootstraps(train_data, times = 1), #normally would have your crossvalidation info here - something like this (pima_folds &lt;- vfold_cv(train_data, v = 5)) - but will learn about this later on.\n  grid      = tree_grid,\n  metrics   = yardstick::metric_set(yardstick::roc_auc, yardstick::accuracy)\n)\n\ntree_res\n\n# Tuning results\n# Bootstrap sampling \n# A tibble: 1 × 4\n  splits            id         .metrics          .notes          \n  &lt;list&gt;            &lt;chr&gt;      &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [537/191]&gt; Bootstrap1 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 4]&gt;\n\n\nInspect the best results (e.g. by ROC AUC):\n\nshow_best(tree_res, metric = \"roc_auc\", n = 5)\n\n# A tibble: 5 × 8\n  tree_depth min_n .metric .estimator  mean     n std_err .config         \n       &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n1          7    30 roc_auc binary     0.742     1      NA pre0_mod20_post0\n2         10    30 roc_auc binary     0.742     1      NA pre0_mod25_post0\n3          3     2 roc_auc binary     0.734     1      NA pre0_mod06_post0\n4          3     9 roc_auc binary     0.734     1      NA pre0_mod07_post0\n5          3    16 roc_auc binary     0.734     1      NA pre0_mod08_post0\n\n\n\ntree_metrics &lt;- collect_metrics(tree_res) %&gt;%\n  filter(.metric == \"roc_auc\")\n\ntree_metrics\n\n# A tibble: 25 × 8\n   tree_depth min_n .metric .estimator  mean     n std_err .config         \n        &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1          1     2 roc_auc binary     0.730     1      NA pre0_mod01_post0\n 2          1     9 roc_auc binary     0.730     1      NA pre0_mod02_post0\n 3          1    16 roc_auc binary     0.730     1      NA pre0_mod03_post0\n 4          1    23 roc_auc binary     0.730     1      NA pre0_mod04_post0\n 5          1    30 roc_auc binary     0.730     1      NA pre0_mod05_post0\n 6          3     2 roc_auc binary     0.734     1      NA pre0_mod06_post0\n 7          3     9 roc_auc binary     0.734     1      NA pre0_mod07_post0\n 8          3    16 roc_auc binary     0.734     1      NA pre0_mod08_post0\n 9          3    23 roc_auc binary     0.733     1      NA pre0_mod09_post0\n10          3    30 roc_auc binary     0.725     1      NA pre0_mod10_post0\n# ℹ 15 more rows\n\n\n\nautoplot(tree_res)\n\n\n\n\n\n\n\nSelect the best hyperparameters and finalise the workflow:\n\nbest_tree &lt;- select_best(tree_res, metric = \"roc_auc\")\nbest_tree\n\n# A tibble: 1 × 3\n  tree_depth min_n .config         \n       &lt;int&gt; &lt;int&gt; &lt;chr&gt;           \n1          7    30 pre0_mod20_post0\n\n\n\nfinal_tree_wf &lt;- finalize_workflow(tree_wf_cv, best_tree)\n\nfinal_tree_fit &lt;- final_tree_wf %&gt;%\n  fit(data = train_data)\n\nfinal_tree_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 537 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 537 187 neg (0.34823091 0.65176909)  \n    2) glucose&gt;=127.5 189  71 pos (0.62433862 0.37566138)  \n      4) mass&gt;=29.95 142  39 pos (0.72535211 0.27464789)  \n        8) glucose&gt;=157.5 62   6 pos (0.90322581 0.09677419) *\n        9) glucose&lt; 157.5 80  33 pos (0.58750000 0.41250000)  \n         18) pressure&lt; 61 12   0 pos (1.00000000 0.00000000) *\n         19) pressure&gt;=61 68  33 pos (0.51470588 0.48529412)  \n           38) age&gt;=30.5 42  14 pos (0.66666667 0.33333333)  \n             76) pedigree&gt;=0.415 25   4 pos (0.84000000 0.16000000) *\n             77) pedigree&lt; 0.415 17   7 neg (0.41176471 0.58823529) *\n           39) age&lt; 30.5 26   7 neg (0.26923077 0.73076923) *\n      5) mass&lt; 29.95 47  15 neg (0.31914894 0.68085106)  \n       10) age&gt;=27.5 31  14 neg (0.45161290 0.54838710)  \n         20) insulin&gt;=69.5 13   4 pos (0.69230769 0.30769231) *\n         21) insulin&lt; 69.5 18   5 neg (0.27777778 0.72222222) *\n       11) age&lt; 27.5 16   1 neg (0.06250000 0.93750000) *\n    3) glucose&lt; 127.5 348  69 neg (0.19827586 0.80172414)  \n      6) age&gt;=28.5 154  53 neg (0.34415584 0.65584416)  \n       12) mass&gt;=26.35 128  52 neg (0.40625000 0.59375000)  \n         24) pedigree&gt;=0.561 35  12 pos (0.65714286 0.34285714)  \n           48) glucose&gt;=101 23   5 pos (0.78260870 0.21739130) *\n           49) glucose&lt; 101 12   5 neg (0.41666667 0.58333333) *\n         25) pedigree&lt; 0.561 93  29 neg (0.31182796 0.68817204)  \n           50) triceps&lt; 27.5 54  22 neg (0.40740741 0.59259259)  \n            100) glucose&gt;=93.5 44  22 pos (0.50000000 0.50000000)  \n              200) pressure&lt; 74.5 25   9 pos (0.64000000 0.36000000) *\n              201) pressure&gt;=74.5 19   6 neg (0.31578947 0.68421053) *\n            101) glucose&lt; 93.5 10   0 neg (0.00000000 1.00000000) *\n           51) triceps&gt;=27.5 39   7 neg (0.17948718 0.82051282) *\n       13) mass&lt; 26.35 26   1 neg (0.03846154 0.96153846) *\n      7) age&lt; 28.5 194  16 neg (0.08247423 0.91752577) *\n\n\n5. Evaluate the tuned tree on the test set\n\ntrain_preds_final &lt;- predict(final_tree_fit, train_data, type = \"prob\") %&gt;%\n  bind_cols(predict(final_tree_fit, train_data, type = \"class\")) %&gt;%\n  bind_cols(train_data)\n\ntest_preds_final &lt;- predict(final_tree_fit, test_data, type = \"prob\") %&gt;%\n  bind_cols(predict(final_tree_fit, test_data, type = \"class\")) %&gt;%\n  bind_cols(test_data)\n\n\ntrain_metrics_final &lt;- train_preds_final %&gt;%\n  class_metrics(truth = diabetes, estimate = .pred_class)\n\ntest_metrics_final &lt;- test_preds_final %&gt;%\n  class_metrics(truth = diabetes, estimate = .pred_class)\n\ntrain_metrics_final\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.845\n2 precision   binary         0.825\n3 recall      binary         0.706\n4 sens        binary         0.706\n5 specificity binary         0.92 \n\ntest_metrics_final\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.762\n2 precision   binary         0.703\n3 recall      binary         0.556\n4 sens        binary         0.556\n5 specificity binary         0.873\n\n\nWe can also compare ROC AUC:\n\nroc_auc(train_preds_final, truth = diabetes, .pred_pos)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.886\n\nroc_auc(test_preds_final,  truth = diabetes, .pred_pos)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.816\n\n\nAnd visualise the final tree:\n\nfinal_tree_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  pluck(\"fit\") %&gt;%\n  rpart.plot(extra = 101)\n\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\n\n\nKey messages\n\nA basic tree can easily overfit: high training accuracy, lower test accuracy.\n\n\nHyperparameters like tree_depth and min_n control complexity and help reduce overfitting.\n\n\ntidymodels lets us:\n\ndefine tunable specifications (tune()),\nkeep preprocessing and modelling together in workflows,\nand only evaluate on the test set once at the very end.\n\n\n\nRemember:\n&gt; Hyperparameter tuning is part of the training process.\n&gt; The test set must never be used to choose hyperparameters – only to evaluate the final chosen model.",
    "crumbs": [
      "Overfitting, DT and RF",
      "Decision Trees in Classification (tidymodels)"
    ]
  }
]